{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrepancy Modeling for Epidemiology\n",
    "\n",
    "This part of the homework focuses on discrepancy modeling to refine an ODE-based model that partially captures the real-world COVID-19 dynamics at the population level. You will implement a simplified version of the APHYNITY framework discussed in class. Using real-world data from Washtenaw County, MI, we will model the spread of COVID-19 during the Winter of 2023.\n",
    "\n",
    "The following libraries will be necessary for your implementation. Ensure you have them installed before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import scipy.integrate # SciPy module for integrating ODEs\n",
    "solve_ivp = scipy.integrate.solve_ivp\n",
    "from torchdiffeq import odeint  # Solver for ODEs integrated with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Discrepancy Modeling with Vanilla Neural ODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SEIR (Susceptible - Exposed - Infectious - Recovered) model is a compartmental model used in epidemiology to describe the progression of infectious diseases within a population. It divides the population into four compartments, each representing a stage of the disease. The transitions between compartments are governed by the following differential equations.\n",
    "\n",
    "\\begin{split}\\begin{aligned}\n",
    "\\frac{dS}{dt} & = -\\frac{\\beta SI}{N}\\\\\n",
    "\\frac{dE}{dt} & = \\frac{\\beta SI}{N} - \\sigma E\\\\\n",
    "\\frac{dI}{dt} & = \\sigma E - \\gamma I\\\\\n",
    "\\frac{dR}{dt} & = \\gamma I\n",
    "\\end{aligned}\\end{split}\n",
    "where $N = S + E + I + R$ is the total population.\n",
    "<div align=\"center\">\n",
    "   <img src=\"seir.png\" width=\"900\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compartments in the SEIR Model:**\n",
    "* Susceptible (S): Represents individuals who are not yet infected but are at risk of infection.\n",
    "* Exposed (E): Represents individuals who have been exposed to the infection but are not yet infectious.\n",
    "* Infectious (I): Represents individuals who are infected and can transmit the disease to susceptible individuals.\n",
    "* Recovered (R): Represents individuals who have recovered from the disease and are assumed to have immunity. In some variations of the model, this compartment can also include individuals who died from the disease (R = Recovered/Removed).\n",
    "\n",
    "**Parameters:**\n",
    "* $\\beta$ : Transmission rate (how many people one infected individual infects per unit time).\n",
    "* $\\sigma$ : Rate at which exposed individuals become infectious ( 1 / incubation period ).\n",
    "* $\\gamma$ : Recovery rate ( 1 / infectious period ).\n",
    "\n",
    "In our experiments, we will use the reproduction number which is defined as $R_0 = \\beta / \\gamma$; it represents the average number of secondary infections caused by a single infectious individual in a fully susceptible population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1: Complete the SEIR model implementation**\n",
    "* Implement the differential equations of the SEIR model\n",
    "* Use the ODE parameters in `params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEIR(nn.Module):\n",
    "    \"\"\"\n",
    "    SEIR model for epidemiological modeling, commonly used for diseases like COVID-19.\n",
    "\n",
    "    Args:\n",
    "        N (int): Total population size.\n",
    "        Rt (float): Reproduction number.\n",
    "        beta_init (float): Initial value for the transmission rate parameter.\n",
    "        sigma_init (float): Initial value for the transition rate parameter.\n",
    "        reporting_rate (float): Fraction of cases reported, default is 0.025.\n",
    "    \"\"\"\n",
    "    def __init__(self, N, Rt, beta_init, sigma_init, reporting_rate=0.025):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Rt = Rt\n",
    "        self.N = N\n",
    "        self.reporting_rate = reporting_rate\n",
    "\n",
    "        # Parameter transformation for bounded parameters in the range (0, 1)\n",
    "        EPS = -1e-12\n",
    "        self.beta = Parameter(torch.tensor(np.arctanh(1/.5*beta_init - 1 + EPS), dtype=torch.float32))\n",
    "        self.sigma = Parameter(torch.tensor(np.arctanh(1/.5*sigma_init - 1 + EPS), dtype=torch.float32))\n",
    "\n",
    "    def get_scaled_params(self, convert_cpu=False):\n",
    "        \"\"\"\n",
    "        Converts real-value parameters to scaled values in the range (0, 1).\n",
    "\n",
    "        Args:\n",
    "            convert_cpu (bool): If True, detach and convert parameters to CPU for visualization.\n",
    "\n",
    "        Returns:\n",
    "            dict: Scaled model parameters ('beta', 'sigma', 'gamma').\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        # these take values in the domain 0-1\n",
    "        params['beta'] = .5 * (torch.tanh(self.beta) + 1)\n",
    "        params['sigma']  = .5 * (torch.tanh(self.sigma) + 1)\n",
    "        params['gamma'] = params['beta'] / self.Rt\n",
    "\n",
    "        # for printing and saving results, detach and send to cpu\n",
    "        if convert_cpu:\n",
    "            for k, v in params.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    params[k] = v.detach().cpu().data.item()\n",
    "        return params\n",
    "    \n",
    "\n",
    "    def forward(self, t, state):\n",
    "        \"\"\"\n",
    "        Computes the ODE derivatives for the SEIR model.\n",
    "\n",
    "        Args:\n",
    "            t (float): Current time.\n",
    "            state (Tensor): Current state values (S, E, I, R).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Derivatives of the state values.\n",
    "        \"\"\"\n",
    "        params = self.get_scaled_params()\n",
    "        \n",
    "        # TODO: Implement SEIR equations \n",
    "        dS_dt  = ...\n",
    "        dE_dt  = ...\n",
    "        dI_dt = ...\n",
    "        dR_dt  = ...\n",
    "\n",
    "        dstate_dt = torch.stack([dS_dt, dE_dt, dI_dt, dR_dt], 0)\n",
    "\n",
    "        return dstate_dt\n",
    "    \n",
    "    def new_reported_cases(self, E):\n",
    "        \"\"\"\n",
    "        Computes the number of newly reported cases, accounting for underreporting.\n",
    "\n",
    "        Args:\n",
    "            E (Tensor): Number of exposed individuals.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Newly reported cases.\n",
    "        \"\"\"\n",
    "        new_cases = self.get_scaled_params()['sigma'] * E\n",
    "        return self.reporting_rate * new_cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2: Complete the initial conditions for the SEIR model**\n",
    "* Implement the initial conditions for SEIR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialConditions(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable initial conditions for the SEIR model states (S0, E0, I0, R0).\n",
    "\n",
    "    Args:\n",
    "        N (int): Total population size.\n",
    "        E0_init (float): Initial exposed population.\n",
    "        I0_init (float): Initial infectious population.\n",
    "        R0_init (float): Initial recovered population.\n",
    "    \"\"\"\n",
    "    def __init__(self, N, E0_init, I0_init, R0_init):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.E0 = Parameter(torch.tensor(E0_init, dtype=torch.float32))\n",
    "        self.I0 = Parameter(torch.tensor(I0_init, dtype=torch.float32))\n",
    "        self.R0 = Parameter(torch.tensor(R0_init, dtype=torch.float32))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Computes the initial susceptible population (S0) based on total population and initial conditions.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Initial state values [S0, E0, I0, R0].\n",
    "        \"\"\"\n",
    "        # TODO: Complete construction of initial conditions\n",
    "        S0 = ...\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3: Instantiate the SEIR model and its initial conditions**\n",
    "* We are giving you real-world data for COVID-19 spread in Washtenaw county, MI for Winter 2023.\n",
    "* We also give you some already learned parameters and initial conditions; use them in your instantiations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data and preprocess\n",
    "df = pd.read_csv('Washtenaw_data.csv', header=0)\n",
    "# Smooth data with a 7-day moving average\n",
    "cases = df['Cases'].rolling(7, min_periods=1).mean().to_numpy()\n",
    "cases = torch.tensor(cases, dtype=torch.float32)\n",
    "y_len = len(cases)\n",
    "\n",
    "# Model initialization\n",
    "beta = 0.33\n",
    "Rt = 1.19\n",
    "sigma = 0.48\n",
    "E0, I0, R0 = 600, 800, 1e4\n",
    "POP_SIZE = 372258 # population size\n",
    "\n",
    "# TODO: Instantiate the SEIR model and initial conditions\n",
    "model_phy = SEIR(...)\n",
    "init_conditions = InitialConditions(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize how well our SEIR model fit the real-world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data using your fitted SEIR model\n",
    "time_points = torch.linspace(0, y_len, y_len)\n",
    "initial_conditions = init_conditions.forward()  # Initial state [S0, E0, I0, R0]\n",
    "states = odeint(model_phy, initial_conditions, time_points, method='rk4')  # Solve ODE\n",
    "\n",
    "# Extract each compartment, calculate proportions\n",
    "susceptible = states[:, 0].detach().numpy() / POP_SIZE\n",
    "exposed = states[:, 1].detach().numpy() / POP_SIZE\n",
    "infectious = states[:, 2].detach().numpy()/ POP_SIZE\n",
    "recovered = states[:, 3].detach().numpy() / POP_SIZE\n",
    "reported_cases = model_phy.new_reported_cases(E = states[:, 1])\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4.3))  # 1 row, 2 columns of subplots\n",
    "\n",
    "# First subplot: SEIR compartments\n",
    "ax[0].plot(time_points.numpy(), susceptible, label='Susceptible Population', color='blue')\n",
    "ax[0].plot(time_points.numpy(), exposed, label='Exposed Population', color='orange')\n",
    "ax[0].plot(time_points.numpy(), infectious, label='Infectious Population', color='green')\n",
    "ax[0].plot(time_points.numpy(), recovered, label='Recovered Population', color='red')\n",
    "ax[0].set_title('SEIR Model')\n",
    "ax[0].set_xlabel('Time')\n",
    "ax[0].set_ylabel('Population Fraction')\n",
    "ax[0].legend(loc='best')\n",
    "ax[0].grid()\n",
    "\n",
    "# Second subplot: Reported vs Predicted Cases\n",
    "ax[1].plot(cases, label='Actual Cases', color='blue')\n",
    "ax[1].plot(reported_cases.detach().numpy(), label='Predicted Cases', color='green')\n",
    "ax[1].set_title('Reported vs Predicted Cases')\n",
    "ax[1].set_xlabel('Time')\n",
    "ax[1].set_ylabel('Cases')\n",
    "ax[1].legend(loc='best')\n",
    "ax[1].grid()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this is OK but not yet a great fit. Let's implement the APHYNITY framework with a vanilla Neural ODE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODE(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural ODE for modeling the time derivative of states (dx/dt).\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Input dimensionality.\n",
    "        hidden_dim (int): Hidden layer dimensionality.\n",
    "        output_dim (int): Output dimensionality.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, output_dim, bias=None)\n",
    "        )\n",
    "        self._initialize_weights(self.net)\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        \"\"\"\n",
    "        Initializes the weights of the neural network using Xavier initialization.\n",
    "\n",
    "        Args:\n",
    "            module (nn.Module): Neural network module.\n",
    "        \"\"\"\n",
    "        for layer in module:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        \"\"\"\n",
    "        Computes the time derivative of the input state.\n",
    "\n",
    "        Args:\n",
    "            t (float): Current time step.\n",
    "            x (Tensor): Current state.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Time derivative (dx/dt).\n",
    "        \"\"\"\n",
    "        dx_dt = self.net(x)\n",
    "        return dx_dt\n",
    "    \n",
    "    def get_derivative_dataset(self, t_set, x_set):\n",
    "        \"\"\"\n",
    "        Handles derivatives for multiple data points.\n",
    "\n",
    "        Args:\n",
    "            t_set (Tensor): Set of time steps.\n",
    "            x_set (Tensor): Set of states.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Derivatives for all data points.\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        for i in range(t_set.size(0)):\n",
    "            res.append(self.forward(t_set[i], x_set[i, :]))\n",
    "        return torch.vstack(res)\n",
    "\n",
    "class DerivativeEstimator(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines a physics-based model and a data-driven augmentation component.\n",
    "\n",
    "    Args:\n",
    "        model_phy (nn.Module): Incomplete physics-based model.\n",
    "        model_aug (nn.Module): Data-driven augmentation component.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_phy, model_aug):\n",
    "        super().__init__()\n",
    "        self.model_phy = model_phy\n",
    "        self.model_aug = model_aug\n",
    "\n",
    "    def forward(self, t, state):\n",
    "        \"\"\"\n",
    "        Combines predictions from the physics-based model and augmentation model.\n",
    "\n",
    "        Args:\n",
    "            t (float): Current time step.\n",
    "            state (Tensor): Current state.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Combined derivatives.\n",
    "        \"\"\"\n",
    "        res_phy = self.model_phy(t, state)\n",
    "        res_aug = self.model_aug(t, state)\n",
    "        return res_phy + res_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4: Instantiate the NeuralODE and DerivativeEstimator**\n",
    "* `DerivativeEstimator` class will combine both the physical model and the data-driven model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will only train the data-driven model\n",
    "for param in model_phy.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in init_conditions.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Data-driven augmentation model\n",
    "model_aug = NeuralODE(...)\n",
    "combined_model = DerivativeEstimator(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5: Implement APHYNITY training for our combined model**\n",
    "* Solve ODE for combined model\n",
    "* Compute derivatives with the augmentation model\n",
    "* Predicted reported cases\n",
    "* Complete update on lambda\n",
    "* Implement APHYNITY loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop constants\n",
    "LAMBDA = 1e-1\n",
    "TAU = 1e-3\n",
    "EPSILON = 1e-5\n",
    "\n",
    "def update_lambda(lambda_value, loss):\n",
    "    \"\"\"\n",
    "    Updates the lambda parameter based on the loss.\n",
    "\n",
    "    Args:\n",
    "        lambda_value (float): Current lambda parameter.\n",
    "        loss (Tensor): Current loss value.\n",
    "\n",
    "    Returns:\n",
    "        float: Updated lambda parameter.\n",
    "    \"\"\"\n",
    "    return lambda_value + TAU * loss.detach().cpu().item()\n",
    "\n",
    "def train_model(model):\n",
    "    \"\"\" Function to train our models \"\"\"\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    params = list(model.model_aug.parameters()) # train only data-driven component\n",
    "    optimizer = optim.RMSprop(params, lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=250)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Set maximum training iterations and print frequency for logging\n",
    "    max_epochs = 1000\n",
    "    print_every = 50\n",
    "    lambda_param = LAMBDA\n",
    "\n",
    "    # Initial conditions\n",
    "    y0 = init_conditions.forward()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(max_epochs+1):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # TODO: Solve the ODE\n",
    "        states = ...\n",
    "\n",
    "        # TODO: Compute derivatives with the augmentation model\n",
    "        aug_derivatives = ...\n",
    "        loss_aug = ((aug_derivatives.norm(p=2, dim=1) / (states.norm(p=2, dim=1) + EPSILON)) ** 2).mean()\n",
    "        \n",
    "        # Predicted reported cases and trajectory loss\n",
    "        # TODO: Use the right argument to the function\n",
    "        pred_traj = model.model_phy.new_reported_cases(...)\n",
    "        traj_loss = loss_fn(pred_traj, cases)\n",
    "\n",
    "        # Total loss\n",
    "        # TODO: Implement APHYNITY loss\n",
    "        total_loss = ...\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch}: Loss Aug: {loss_aug.item():.4e}, \"\n",
    "                f\"Trajectory Loss (sqrt): {traj_loss.sqrt().item():.4f}, \"\n",
    "                f\"Learning Rate: {scheduler.get_last_lr()[0]:.1e}, \"\n",
    "                f\"Lambda: {lambda_param:.3f}\"\n",
    "            )\n",
    "            # TODO: Complete update on lambda\n",
    "            lambda_param = update_lambda(...)\n",
    "\n",
    "            # Break if learning rate is too small\n",
    "            if scheduler.get_last_lr()[0] < 1e-4:\n",
    "                break\n",
    "\n",
    "        # Backpropagation\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params, 1e-1)\n",
    "        optimizer.step()\n",
    "        scheduler.step(total_loss)\n",
    "\n",
    "    return pred_traj\n",
    "        \n",
    "pred_traj_node = train_model(combined_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our new fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4.3))  \n",
    "\n",
    "# Reported vs Predicted Cases\n",
    "ax.plot(cases, label='Actual Cases', color='blue')\n",
    "ax.plot(pred_traj_node.detach().numpy(), label='Predicted Cases', color='green')\n",
    "ax.set_title('Reported vs Predicted Cases')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Cases')\n",
    "ax.legend(loc='best')\n",
    "ax.grid()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Improving combined model with time-aware Neural ODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we observed that the fitting of the Neural ODE was not yet up to standard. One way to improve this is by enhancing expressiveness through the incorporation of time embeddings, which we will implement as positional embeddings. If you do not know what a position embedding is, you can see https://kazemnejad.com/blog/transformer_architecture_positional_encoding/.\n",
    "\n",
    "**Task 6: Use the `PositionalEmbedding` module in your Neural ODE**\n",
    "* Create any modules you may need\n",
    "* Update your forward function\n",
    "* Aim for a trajectory loss (sqrt) of less than 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes positional information for time steps using sinusoidal embeddings.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): Dimensionality of embeddings.\n",
    "        max_len (int): Maximum length of time series. Default is 1000.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        # Initialize sinusoidal encoding\n",
    "        pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n",
    "        # pe.require_grad = False\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Register buffer to avoid tracking gradients\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Returns the positional encoding for the input position.\n",
    "\n",
    "        Args:\n",
    "            x (int): Position index.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Positional encoding vector.\n",
    "        \"\"\"\n",
    "        return self.pe[int(x), :]\n",
    "\n",
    "\n",
    "class TimeNeuralODE(NeuralODE):\n",
    "    \"\"\"\n",
    "    Neural ODE with positional encoding for time.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Input dimensionality.\n",
    "        hidden_dim (int): Hidden layer dimensionality.\n",
    "        output_dim (int): Output dimensionality.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "        # TODO: Instantiate the positional embedding and other modules you may need\n",
    "        ...\n",
    "\n",
    "        # TODO: Initialize weights for new modules if needed\n",
    "        ...\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        \"\"\"\n",
    "        Computes the time derivative of the input state.\n",
    "\n",
    "        Args:\n",
    "            t (float): Current time step.\n",
    "            x (Tensor): Current state.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Time derivative (dx/dt).\n",
    "        \"\"\"\n",
    "        # TODO: Incorporate your position embedding\n",
    "        dx_dt = ...\n",
    "        return dx_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate the updated NeuralODE and DerivativeEstimator, as done previously, to allow for a direct comparison\n",
    "model_aug = TimeNeuralODE(...)\n",
    "combined_model = DerivativeEstimator(...)\n",
    "pred_traj_time_node = train_model(combined_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the fitting for this improved Neural ODE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4.3))  \n",
    "\n",
    "# Reported vs Predicted Cases\n",
    "ax.plot(cases, label='Actual Cases', color='blue')\n",
    "ax.plot(pred_traj_time_node.detach().numpy(), label='Predicted Cases', color='green')\n",
    "ax.set_title('Reported vs Predicted Cases')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Cases')\n",
    "ax.legend(loc='best')\n",
    "ax.grid()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7: Comment on the results**\n",
    "\n",
    "Address the following points:\n",
    "* Reflect on the trade-offs between interpretability (from the physics-based component) and flexibility (from the neural network component). In which scenarios would you prefer each of the models we tested?\n",
    "* Describe the role of the positional embeddings and how they enhance the expressiveness of the Neural ODE. Could other types of embeddings or feature transformations be explored?\n",
    "* In this exercise, we fitted COVID-19 data. What changes would you make if this model were to be used for forecasting (predict the future)? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse598",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
