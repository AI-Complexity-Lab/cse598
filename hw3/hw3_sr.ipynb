{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic Regression with NeSymReS\n",
    "\n",
    "In this part of the homework, you will explore the implementation and evaluation of Neural Symbolic Regression that Scales (NeSymReS), an approach to symbolic regression that leverages neural networks for predicting mathematical equations from data. Symbolic regression combines machine learning and symbolic computation to discover interpretable mathematical expressions that best describe given data. As we discussed in class, NeSymReS achieves this by integrating a transformer-based neural network for sequence generation with the BFGS optimization algorithm to refine coefficients in predicted equations. For details, see **Neural Symbolic Regression That Scales**.  [[`arXiv`](https://arxiv.org/pdf/2106.06427.pdf)] \n",
    "\n",
    "\n",
    "The following libraries will be necessary for your implementation. Ensure you have them installed before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import omegaconf\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from sympy import lambdify \n",
    "import sympy as sp\n",
    "from generator import Generator, add_additive_constants, add_multiplicative_constants\n",
    "from data import *\n",
    "from scipy.optimize import minimize\n",
    "from models import SetEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1: Compute the symbolic loss**\n",
    "* Evaluate the predicted equation for each data point and calculating the difference from the target.\n",
    "\n",
    "**Task 2: Reconstruct optimized expression**\n",
    "* Replace placeholders with optimized constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamHypotheses:\n",
    "    \"\"\"\n",
    "    A class to manage and store beam search hypotheses.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_hyp, max_len, length_penalty, early_stopping):\n",
    "        \"\"\"\n",
    "        Initialize the hypotheses object.\n",
    "\n",
    "        Args:\n",
    "            n_hyp (int): Number of hypotheses to keep.\n",
    "            max_len (int): Maximum sequence length.\n",
    "            length_penalty (float): Penalty for longer sequences.\n",
    "            early_stopping (bool): Whether to stop once enough hypotheses are found.\n",
    "        \"\"\"\n",
    "        self.n_hyp = n_hyp\n",
    "        self.max_len = max_len\n",
    "        self.length_penalty = length_penalty\n",
    "        self.early_stopping = early_stopping\n",
    "        self.hypotheses = []\n",
    "        self.worst_score = float(\"inf\")\n",
    "\n",
    "    def add(self, hypothesis, score):\n",
    "        \"\"\"\n",
    "        Add a new hypothesis with its score.\n",
    "\n",
    "        Args:\n",
    "            hypothesis (list): The hypothesis sequence.\n",
    "            score (float): The score of the hypothesis.\n",
    "        \"\"\"\n",
    "        normalized_score = score / (len(hypothesis) ** self.length_penalty)\n",
    "        if len(self.hypotheses) < self.n_hyp or normalized_score > self.worst_score:\n",
    "            self.hypotheses.append((normalized_score, hypothesis))\n",
    "            if len(self.hypotheses) > self.n_hyp:\n",
    "                # Remove the worst-scoring hypothesis\n",
    "                self.hypotheses.sort(key=lambda x: x[0])\n",
    "                self.hypotheses.pop(0)\n",
    "                self.worst_score = self.hypotheses[0][0]\n",
    "            else:\n",
    "                self.worst_score = min(normalized_score, self.worst_score)\n",
    "    \n",
    "class TimedFun:\n",
    "    def __init__(self, fun, stop_after=10):\n",
    "        self.fun_in = fun\n",
    "        self.started = False\n",
    "        self.stop_after = stop_after\n",
    "\n",
    "    def fun(self, x, *args):\n",
    "        if self.started is False:\n",
    "            self.started = time.time()\n",
    "        elif abs(time.time() - self.started) >= self.stop_after:\n",
    "            raise ValueError(\"Time is over.\")\n",
    "        self.fun_value = self.fun_in(*x, *args)\n",
    "        self.x = x\n",
    "        return self.fun_value\n",
    "    \n",
    "def bfgs_fcn(pred_str, X, y, cfg):\n",
    "    \"\"\"\n",
    "    Optimize coefficients of a predicted equation using BFGS.\n",
    "\n",
    "    Args:\n",
    "        pred_str (list): Predicted tokenized equation (prefix format).\n",
    "        X (torch.Tensor): Input variables (shape: [batch_size, num_points, num_variables]).\n",
    "        y (torch.Tensor): Target values (shape: [batch_size, num_points]).\n",
    "        cfg: Configuration object with required settings.\n",
    "\n",
    "    Returns:\n",
    "        best_func (sympy.Expr): Optimized symbolic expression.\n",
    "        best_coeffs (list): Optimized coefficients.\n",
    "        best_loss (float): Loss associated with the best coefficients.\n",
    "        expr (str): Equation structure with placeholders for coefficients.\n",
    "    \"\"\"\n",
    "    # Prepare input data\n",
    "    y = y.squeeze()\n",
    "    X = X.clone()\n",
    "\n",
    "    # Replace unused dimensions with 1 to avoid numerical issues\n",
    "    bool_dim = (X == 0).all(axis=1).squeeze()\n",
    "    X[:, :, bool_dim] = 1\n",
    "\n",
    "    # Decode predicted structure\n",
    "    pred_str = pred_str[1:].tolist()\n",
    "    raw = de_tokenize(pred_str, cfg.id2word)\n",
    "\n",
    "    # Add missing coefficients if needed\n",
    "    if cfg.bfgs.add_coefficients_if_not_existing and 'constant' not in raw:\n",
    "        print(\"No constants in predicted expression. Adding them.\")\n",
    "        variables = {x: sp.Symbol(x, real=True, nonzero=True) for x in cfg.total_variables}\n",
    "        infix = Generator.prefix_to_infix(raw, coefficients=cfg.total_coefficients, variables=cfg.total_variables)\n",
    "        s = Generator.infix_to_sympy(infix, variables, cfg.rewrite_functions)\n",
    "        placeholder = {x: sp.Symbol(x, real=True, nonzero=True) for x in [\"cm\", \"ca\"]}\n",
    "        s = add_multiplicative_constants(s, placeholder[\"cm\"], unary_operators=cfg.una_ops)\n",
    "        s = add_additive_constants(s, placeholder, unary_operators=cfg.una_ops)\n",
    "        s = s.subs(placeholder[\"cm\"], 0.43).subs(placeholder[\"ca\"], 0.421)\n",
    "        s_simplified = constants_to_placeholder(s, symbol=\"constant\")\n",
    "        prefix = Generator.sympy_to_prefix(s_simplified)\n",
    "        candidate = Generator.prefix_to_infix(prefix, coefficients=[\"constant\"], variables=cfg.total_variables)\n",
    "    else:\n",
    "        candidate = Generator.prefix_to_infix(raw, coefficients=[\"constant\"], variables=cfg.total_variables)\n",
    "\n",
    "    # Replace \"constant\" placeholders with unique symbols\n",
    "    candidate = candidate.format(constant=\"constant\")\n",
    "    expr = candidate\n",
    "    for i in range(candidate.count(\"constant\")):\n",
    "        expr = expr.replace(\"constant\", f\"c{i}\", 1)\n",
    "\n",
    "    print(\"Constructing BFGS loss...\")\n",
    "\n",
    "    # Compute the symbolic loss by evaluating the predicted equation\n",
    "    # for each data point and calculating the difference from the target.\n",
    "    diffs = []\n",
    "    for i in range(X.shape[1]):\n",
    "        curr_expr = expr\n",
    "        for idx, var in enumerate(cfg.total_variables):\n",
    "            # TODO: Evaluate symbolic expression\n",
    "            # Hint: Use sympy's `subs` to substitute variable values into the expression.\n",
    "            curr_expr = ...\n",
    "        # TODO: Save the difference between the symbolic evaluation and actual values\n",
    "        diff = ...\n",
    "        diffs.append(diff)\n",
    "\n",
    "    loss = np.mean(np.square(diffs))\n",
    "\n",
    "    print(\"Loss constructed. Starting BFGS optimization...\")\n",
    "\n",
    "    # Optimize coefficients using BFGS\n",
    "    F_loss = []\n",
    "    consts_ = []\n",
    "    funcs = []\n",
    "    symbols = {i: sp.Symbol(f'c{i}') for i in range(candidate.count(\"constant\"))}\n",
    "\n",
    "    for _ in range(cfg.bfgs.n_restarts):\n",
    "        x0 = np.random.randn(len(symbols))  # Initial guess for coefficients\n",
    "        s = list(symbols.values())\n",
    "        fun_timed = TimedFun(fun=sp.lambdify(s, loss, modules=['numpy']), stop_after=cfg.bfgs.stop_time)\n",
    "\n",
    "        # Run BFGS optimization\n",
    "        try:\n",
    "            minimize(fun_timed.fun, x0, method='BFGS')\n",
    "            consts_.append(fun_timed.x)\n",
    "        except Exception as e:\n",
    "            print(f\"BFGS optimization failed: {e}\")\n",
    "            consts_.append([])\n",
    "\n",
    "        # Reconstruct optimized expression\n",
    "        # Hint: `s` is a list of SymPy symbols representing the placeholders (e.g., `c0`, `c1`, ...).\n",
    "        #       This loop iterates over each symbol in `s` and replaces it with the corresponding optimized value\n",
    "        #       from `fun_timed.x[i]`.\n",
    "        # Example:\n",
    "        # If `expr` = \"c0 + c1*x\", then `s = [c0, c1]` and `fun_timed.x = [2.5, 1.8]`.\n",
    "        # The loop will replace:\n",
    "        #   - `c0` with `2.5`\n",
    "        #   - `c1` with `1.8`\n",
    "        # Final result: \"2.5 + 1.8*x\"\n",
    "        final = expr\n",
    "        for i, sym in enumerate(s):\n",
    "            # TODO: Reconstruct the optimized equation by replacing placeholders with optimized constants.\n",
    "            # Hint: `fun_timed.x[i]` contains the optimized value for the i-th constant in the equation.\n",
    "            #       Use sympy's `replace` method to substitute each constant symbol with its optimized value.\n",
    "            final = ...\n",
    "\n",
    "        funcs.append(final)\n",
    "\n",
    "        # Evaluate final loss\n",
    "        values = {var: X[:, :, idx].cpu() for idx, var in enumerate(cfg.total_variables)}\n",
    "        y_found = sp.lambdify(\",\".join(cfg.total_variables), final)(**values).squeeze(0)\n",
    "        final_loss = np.mean(np.square(y_found - y.cpu()).numpy())\n",
    "        F_loss.append(final_loss)\n",
    "\n",
    "    # Select the best solution\n",
    "    try:\n",
    "        k_best = np.nanargmin(F_loss)\n",
    "    except ValueError:\n",
    "        print(\"All-Nan slice encountered. Selecting default.\")\n",
    "        k_best = 0\n",
    "\n",
    "    return funcs[k_best], consts_[k_best], F_loss[k_best], expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3: Complete the implementation of beam search for symbolic regression**\n",
    "* Implement beam search scoring and selection logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    \"\"\"\n",
    "        Module for symbolic regression using a transformer-based architecture.\n",
    "\n",
    "        This model encodes input data with a set encoder, decodes equations using a transformer decoder, \n",
    "        and predicts symbolic expressions. It supports auto-regressive generation and beam search for inference.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.enc = SetEncoder(cfg)\n",
    "        self.trg_pad_idx = cfg.trg_pad_idx\n",
    "        self.tok_embedding = nn.Embedding(cfg.output_dim, cfg.dim_hidden)\n",
    "        self.pos_embedding = nn.Embedding(cfg.length_eq, cfg.dim_hidden)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=cfg.dim_hidden,\n",
    "            nhead=cfg.num_heads,\n",
    "            dim_feedforward=cfg.dec_pf_dim,\n",
    "            dropout=cfg.dropout,\n",
    "        )\n",
    "        self.decoder_transfomer = nn.TransformerDecoder(decoder_layer, num_layers=cfg.dec_layers)\n",
    "        self.fc_out = nn.Linear(cfg.dim_hidden, cfg.output_dim)\n",
    "        self.cfg = cfg\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        self.dropout = nn.Dropout(cfg.dropout)\n",
    "        self.eq = None\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"\n",
    "        Create masks for the target sequence to prevent attending to future tokens \n",
    "        during training and inference.\n",
    "\n",
    "        Args:\n",
    "            trg (torch.Tensor): Target sequence tensor of shape (batch_size, trg_len).\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - trg_pad_mask (torch.Tensor): Padding mask of shape (batch_size, trg_len), \n",
    "                                            where padded positions are masked with -inf \n",
    "                                            and valid positions are 0.0.\n",
    "                - mask (torch.Tensor): Causal mask of shape (trg_len, trg_len), \n",
    "                                    masking future positions with -inf and \n",
    "                                    allowing only current and past positions.\n",
    "        \"\"\"\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).float()\n",
    "        trg_pad_mask = (\n",
    "            trg_pad_mask.masked_fill(trg_pad_mask == 0, float(\"-inf\"))\n",
    "            .masked_fill(trg_pad_mask == 1, float(0.0))\n",
    "            .type_as(trg)\n",
    "        )\n",
    "        trg_len = trg.shape[1]\n",
    "        mask = (torch.triu(torch.ones(trg_len, trg_len)) == 1).transpose(0, 1)\n",
    "        mask = (\n",
    "            mask.float()\n",
    "            .masked_fill(mask == 0, float(\"-inf\"))\n",
    "            .masked_fill(mask == 1, float(0.0))\n",
    "            .type_as(trg)\n",
    "        )\n",
    "        return trg_pad_mask, mask\n",
    "    \n",
    "    def beam_search(self, enc_src, cfg_params):\n",
    "        \"\"\"\n",
    "        Perform beam search to generate symbolic expressions.\n",
    "\n",
    "        Args:\n",
    "            enc_src (torch.Tensor): Encoded source tensor of shape \n",
    "                                    (beam_size, seq_length, hidden_dim), \n",
    "                                    output from the encoder.\n",
    "            cfg_params: Configuration parameters for beam search, including:\n",
    "                - beam_size (int): Number of beams to maintain during search.\n",
    "                - length_eq (int): Maximum length of the generated equation.\n",
    "                - word2id (dict): Mapping of vocabulary words to IDs.\n",
    "                - id2word (dict): Mapping of IDs to vocabulary words.\n",
    "\n",
    "        Returns:\n",
    "            BeamHypotheses: An object containing the n-best hypotheses (generated equations) \n",
    "                            and their respective scores.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize beam search variables\n",
    "        generated = torch.zeros(\n",
    "            [cfg_params.beam_size, self.cfg.length_eq],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        generated[:, 0] = 1  # Start-of-sequence (BOS) token\n",
    "        cache = {\"slen\": 0}\n",
    "        generated_hyps = BeamHypotheses(cfg_params.beam_size, self.cfg.length_eq, 1.0, 1)\n",
    "\n",
    "        beam_scores = torch.zeros(cfg_params.beam_size, dtype=torch.long)\n",
    "        beam_scores[1:] = -1e9  # Assign low scores to all beams except the first\n",
    "        cur_len = torch.tensor(1, dtype=torch.int64)\n",
    "\n",
    "        # Beam search loop\n",
    "        while cur_len < self.cfg.length_eq:\n",
    "            # Generate target masks\n",
    "            generated_mask1, generated_mask2 = self.make_trg_mask(generated[:, :cur_len])\n",
    "\n",
    "            # Compute positional and token embeddings\n",
    "            pos = self.pos_embedding(\n",
    "                torch.arange(0, cur_len)\n",
    "                .unsqueeze(0)\n",
    "                .repeat(generated.shape[0], 1)\n",
    "                .type_as(generated)\n",
    "            )\n",
    "            te = self.tok_embedding(generated[:, :cur_len])\n",
    "            trg_ = self.dropout(te + pos)\n",
    "\n",
    "            # Decoder forward pass\n",
    "            # NOTE: The decoder processes the input for ALL beams simultaneously.\n",
    "            # Each beam has its own sequence (`generated`) and the decoder predicts\n",
    "            # the next token for every beam at the same time.\n",
    "            output = self.decoder_transfomer(\n",
    "                trg_.permute(1, 0, 2), # Shape: (cur_len, beam_size, hidden_dim)\n",
    "                enc_src.permute(1, 0, 2), # Shape: (seq_len, beam_size, hidden_dim)\n",
    "                generated_mask2.float(), # Attention mask\n",
    "                tgt_key_padding_mask=generated_mask1.bool(), # Padding mask\n",
    "            )\n",
    "            # Output logits for the next token prediction for all beams\n",
    "            output = self.fc_out(output)\n",
    "            output = output.permute(1, 0, 2).contiguous() # Shape: (beam_size, cur_len, vocab_size)\n",
    "\n",
    "            # Extract scores for the current step (last token in the sequence)\n",
    "            scores = F.log_softmax(output[:, -1:, :], dim=-1).squeeze(1)\n",
    "            assert output[:, -1:, :].shape == (cfg_params.beam_size, 1, self.cfg.length_eq)\n",
    "\n",
    "            vocab_size = scores.shape[-1]\n",
    "\n",
    "            # Update scores and find next candidates\n",
    "            # TODO: Combine the scores of the current step with the beam scores.\n",
    "            # Each beam's prediction score is added to its cumulative score.\n",
    "            _scores = ... # (beam_size, vocab_size)\n",
    "\n",
    "            # TODO: Flatten the combined scores for easier top-k selection.\n",
    "            # We flatten because we need to select top-k tokens across ALL beams, not just within each beam.\n",
    "            _scores = ... # (beam_size * vocab_size)\n",
    "\n",
    "            # TODO: Use `torch.topk` to select the top `2 * beam_size` scores.\n",
    "            # Why 2 * beam_size? This ensures diversity and prevents early pruning. \n",
    "            # You will filter out redundant candidates in the next step.\n",
    "            next_scores, next_words = ...\n",
    "\n",
    "            next_sent_beam = []\n",
    "\n",
    "            # Process each candidate for the next step\n",
    "            for idx, value in zip(next_words, next_scores):\n",
    "                beam_id = idx // vocab_size\n",
    "                word_id = idx % vocab_size\n",
    "\n",
    "                # Add completed hypotheses\n",
    "                if word_id == cfg_params.word2id[\"F\"] or cur_len + 1 == self.cfg.length_eq:\n",
    "                    generated_hyps.add(\n",
    "                        generated[beam_id, :cur_len].clone().cpu(),\n",
    "                        value.item(),\n",
    "                    )\n",
    "                else:\n",
    "                    next_sent_beam.append((value, word_id, beam_id))\n",
    "\n",
    "                # Limit to beam size\n",
    "                if len(next_sent_beam) == cfg_params.beam_size:\n",
    "                    break\n",
    "\n",
    "            # Update beam variables for the next step\n",
    "            if len(next_sent_beam) == 0:\n",
    "                next_sent_beam = [(0, self.trg_pad_idx, 0)] * cfg_params.beam_size\n",
    "\n",
    "            assert len(next_sent_beam) == cfg_params.beam_size\n",
    "\n",
    "            beam_scores = torch.tensor([x[0] for x in next_sent_beam])\n",
    "            beam_words = torch.tensor([x[1] for x in next_sent_beam])\n",
    "            beam_idx = torch.tensor([x[2] for x in next_sent_beam])\n",
    "            generated = generated[beam_idx, :]\n",
    "            generated[:, cur_len] = beam_words\n",
    "\n",
    "            for k in cache.keys():\n",
    "                if k != \"slen\":\n",
    "                    cache[k] = (cache[k][0][beam_idx], cache[k][1][beam_idx])\n",
    "\n",
    "            cur_len += 1\n",
    "\n",
    "        return generated_hyps\n",
    "\n",
    "    def fitfunc(self, X, y, cfg_params):\n",
    "        \"\"\"\n",
    "        Perform symbolic regression using beam search and BFGS optimization.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data of shape [Number_of_points, Number_of_features].\n",
    "            y (numpy.ndarray): Target data of shape [Number_of_points].\n",
    "            cfg_params: Configuration parameters for beam search and BFGS.\n",
    "\n",
    "        Returns:\n",
    "            dict: Results including predicted equations, losses, and constants.\n",
    "        \"\"\"\n",
    "        # Prepare input tensors\n",
    "        y = y[:, None]\n",
    "        X = torch.tensor(X).unsqueeze(0)\n",
    "\n",
    "        # Pad input to match required dimensions\n",
    "        if X.shape[2] < self.cfg.dim_input - 1:\n",
    "            pad = torch.zeros(1, X.shape[1], self.cfg.dim_input - X.shape[2] - 1)\n",
    "            X = torch.cat((X, pad), dim=2)\n",
    "        y = torch.tensor(y).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Combine inputs and targets for encoding\n",
    "            encoder_input = torch.cat((X, y), dim=2)\n",
    "            enc_src = self.enc(encoder_input)\n",
    "            \n",
    "            # Expand encoder outputs for beam search\n",
    "            src_enc = enc_src\n",
    "            shape_enc_src = (cfg_params.beam_size,) + src_enc.shape[1:]\n",
    "            enc_src = src_enc.unsqueeze(1).expand((1, cfg_params.beam_size) + src_enc.shape[1:]).contiguous().view(shape_enc_src)\n",
    "\n",
    "            # Beam search\n",
    "            generated_hyps = self.beam_search(enc_src, cfg_params)\n",
    "\n",
    "            # BFGS optimization for the best hypotheses\n",
    "            best_preds_bfgs, best_L_bfgs, L_bfgs, P_bfgs = [], [], [], []\n",
    "            cfg_params.id2word[3] = \"constant\"\n",
    "\n",
    "            for __, ww in sorted(generated_hyps.hypotheses, key=lambda x: x[0], reverse=True):\n",
    "                # import pdb\n",
    "                # pdb.set_trace()\n",
    "                pred_w_c, constants, loss_bfgs, exa = bfgs_fcn(ww, X, y, cfg_params)\n",
    "                P_bfgs.append(str(pred_w_c))\n",
    "                L_bfgs.append(loss_bfgs)\n",
    "\n",
    "            if all(np.isnan(np.array(L_bfgs))):\n",
    "                print(\"Warning: All losses are NaN.\")\n",
    "                L_bfgs = float(\"nan\")\n",
    "                best_L_bfgs = None\n",
    "            else:\n",
    "                best_preds_bfgs.append(P_bfgs[np.nanargmin(L_bfgs)])\n",
    "                best_L_bfgs.append(np.nanmin(L_bfgs))\n",
    "\n",
    "            # Return the output dictionary\n",
    "            output = {\n",
    "                \"all_bfgs_preds\": P_bfgs,\n",
    "                \"all_bfgs_loss\": L_bfgs,\n",
    "                \"best_bfgs_preds\": best_preds_bfgs,\n",
    "                \"best_bfgs_loss\": best_L_bfgs,\n",
    "            }\n",
    "            self.eq = output[\"best_bfgs_preds\"]\n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we give you ready-to-go code for loading pre-trained models and test your code. The code sets up a pipeline for testing and comparing the performance of two models pre-trained on synthetic datasets generated from mathematical equations.\n",
    "\n",
    "We offer two models, \"10M\" and \"100M\". Both are trained with the same parameter configuration in config/config.yaml (which contains details of how models are trained). \"10M\" model is trained with 10 million datasets and \"100M\" model is trained with 100 millions dataset.\n",
    "* Link to 100M: [[`Link`](https://drive.google.com/file/d/1JfVBCkLc2iz9JZ72y2LI6y0_c01ShTRq/view?usp=sharing)]\n",
    "* Link to 10M: [[`Link`](https://drive.google.com/file/d/1LsS08VqhgGaq8E_4VZJ7g_GJS5lEhy68/view?usp=sharing)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_data(eq, n_points, variable_range, eq_variables, total_variables):\n",
    "    \"\"\"\n",
    "    Generate test data for the given equation.\n",
    "    \n",
    "    Args:\n",
    "        eq (str): Target equation as a string (e.g., \"x1*sin(x1) + 0.5\").\n",
    "        n_points (int): Number of test data points.\n",
    "        variable_range (tuple): Min and max range for variables (e.g., (-10, 10)).\n",
    "        eq_variables (list): List of variable names  for specific equation (e.g., [\"x1\", \"x2\"]).\n",
    "        total_variables (list): List of variable names from config file.\n",
    "    \n",
    "    Returns:\n",
    "        X (torch.Tensor): Input variables.\n",
    "        y (np.array): Target outputs.\n",
    "    \"\"\"\n",
    "    n_variables = len(eq_variables)\n",
    "    X = np.random.uniform(variable_range[0], variable_range[1], size=(n_points, len(total_variables)))\n",
    "    X[:,n_variables:] = 0\n",
    "    X_dict = {var: X[:, idx] for idx, var in enumerate(total_variables)}\n",
    "    y = lambdify(total_variables, eq)(**X_dict)\n",
    "    return torch.tensor(X, dtype=torch.float32), np.array(y)\n",
    "\n",
    "def evaluate_model(model, X, y, params_fit):\n",
    "    \"\"\"\n",
    "    Evaluate the model's predictions and compute MSE.\n",
    "    \n",
    "    Args:\n",
    "        model: Pre-trained model for evaluation.\n",
    "        X (torch.Tensor): Input variables.\n",
    "        y (np.array): Target outputs.\n",
    "        params_fit: Configuration for the beam search.\n",
    "    \n",
    "    Returns:\n",
    "        mse (float): Mean Squared Error of predictions.\n",
    "    \"\"\"\n",
    "    # Run model to predict equation\n",
    "    output = model.fitfunc(X, y, cfg_params=params_fit)\n",
    "    predicted_eq = output['best_bfgs_preds'][0]\n",
    "    print(f\"Predicted equation: {predicted_eq}\")\n",
    "\n",
    "    # Generate predictions using the predicted equation\n",
    "    y_pred = lambdify(params_fit.total_variables, predicted_eq)(**{var: X[:, idx].numpy() for idx, var in enumerate(params_fit.total_variables)})\n",
    "\n",
    "    # Calculate Mean Squared Error\n",
    "    mse = np.mean((y - y_pred) ** 2)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    return predicted_eq, mse\n",
    "\n",
    "# Load config files, models, and their parameters \n",
    "with open('./config/eq_setting.json', 'r') as json_file:\n",
    "    eq_setting = json.load(json_file)\n",
    "\n",
    "cfg = omegaconf.OmegaConf.load(\"./config/config.yaml\")\n",
    "\n",
    "# Load the pre-trained models\n",
    "model_10M = Model.load_from_checkpoint(\"./weights/10M.ckpt\", cfg=cfg.architecture)\n",
    "model_100M = Model.load_from_checkpoint(\"./weights/100M.ckpt\", cfg=cfg.architecture)\n",
    "model_10M.eval()\n",
    "model_100M.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_10M.cuda()\n",
    "    model_100M.cuda()\n",
    "\n",
    "# Set up BFGS load rom the hydra config yaml\n",
    "bfgs = BFGSParams(\n",
    "        activated= cfg.inference.bfgs.activated,\n",
    "        n_restarts=cfg.inference.bfgs.n_restarts,\n",
    "        add_coefficients_if_not_existing=cfg.inference.bfgs.add_coefficients_if_not_existing,\n",
    "        normalization_o=cfg.inference.bfgs.normalization_o,\n",
    "        idx_remove=cfg.inference.bfgs.idx_remove,\n",
    "        normalization_type=cfg.inference.bfgs.normalization_type,\n",
    "        stop_time=cfg.inference.bfgs.stop_time,\n",
    "    )\n",
    "\n",
    "# Configuration for beam search\n",
    "cfg_params = FitParams(\n",
    "    word2id=eq_setting[\"word2id\"],\n",
    "    id2word={int(k): v for k, v in eq_setting[\"id2word\"].items()},\n",
    "    una_ops=eq_setting[\"una_ops\"],\n",
    "    bin_ops=eq_setting[\"bin_ops\"],\n",
    "    total_variables=list(eq_setting[\"total_variables\"]),\n",
    "    total_coefficients=list(eq_setting[\"total_coefficients\"]),\n",
    "    rewrite_functions=list(eq_setting[\"rewrite_functions\"]),\n",
    "    bfgs=bfgs,\n",
    "    beam_size=cfg.inference.beam_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the functions to generate the test data and predict a equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_test_data(\"x_1*sin(x_1) + 1/20 * cos(x_1)\", 500, (-5, 5), [\"x_1\"], [\"x_1\", \"x_2\", \"x_3\"])\n",
    "predicted_eq, mse_10M = evaluate_model(model_10M, X, y, cfg_params)\n",
    "print(\"\\nPredicted equation\", predicted_eq)\n",
    "print(\"MSE\", mse_10M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Systematic evaluation of pre-trained SR models\n",
    "\n",
    "Next, we will define several test cases and evaluate the two models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test cases\n",
    "test_cases = [\n",
    "    {\"eq\": \"x_1 * sin(x_1)\", \"n_points\": 500, \"variable_range\": (-10, 10), \"variables\": [\"x_1\"]},\n",
    "    {\"eq\": \"x_1 * exp(x_2) + 0.1\", \"n_points\": 500, \"variable_range\": (-3, 3), \"variables\": [\"x_1\", \"x_2\"]},\n",
    "    {\"eq\": \"x_1**3 - x_2**2 + 0.1 * x_3**2\", \"n_points\": 500, \"variable_range\": (-4, 4), \"variables\": [\"x_1\", \"x_2\", \"x_3\"]},\n",
    "    {\"eq\": \"exp(-x_1) * sin(x_2) + log(abs(x_3) + 1)\", \"n_points\": 500, \"variable_range\": (-2, 2), \"variables\": [\"x_1\", \"x_2\", \"x_3\"]},\n",
    "    {\"eq\": \"tan(x_1) / (1 + x_2**2) + x_3**2\", \"n_points\": 500, \"variable_range\": (-1, 1), \"variables\": [\"x_1\", \"x_2\", \"x_3\"]},\n",
    "]\n",
    "\n",
    "# Evaluate models\n",
    "results = {\"10M\": [], \"100M\": []}\n",
    "\n",
    "for case in test_cases:\n",
    "    print(f\"\\nEvaluating for equation: {case['eq']}\")\n",
    "\n",
    "    # Generate test data\n",
    "    X, y = generate_test_data(case[\"eq\"], case[\"n_points\"], case[\"variable_range\"], case[\"variables\"], eq_setting[\"total_variables\"])\n",
    "\n",
    "    # Evaluate 10M model\n",
    "    print(\"\\n10M Model Results:\")\n",
    "    predicted_eq, mse_10M = evaluate_model(model_10M, X, y, cfg_params)\n",
    "    results[\"10M\"].append({\"eq\": case[\"eq\"], 'pred_eq': predicted_eq, \"mse\": mse_10M})\n",
    "\n",
    "    # Evaluate 100M model\n",
    "    print(\"\\n100M Model Results:\")\n",
    "    predicted_eq, mse_100M = evaluate_model(model_100M, X, y, cfg_params)\n",
    "    results[\"100M\"].append({\"eq\": case[\"eq\"], 'pred_eq': predicted_eq, \"mse\": mse_100M})\n",
    "\n",
    "# Print results summary\n",
    "print(\"\\n--- Results Summary ---\")\n",
    "for model_name, model_results in results.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    for result in model_results:\n",
    "        print(f\"Equation: {result['eq']} | Pred Eq.: {result['pred_eq']} | MSE: {result['mse']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4: Comment on the last results**\n",
    "* Compare methods quantitatively and qualitatively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Exploring the limitations of NeSymReS\n",
    "\n",
    "**Task 5: Break the code**\n",
    "* Your goal is to design equations that cause the model to fail in some way (e.g., high errors, runtime issues, or incorrect predictions). You are encouraged to vary the complexity of equations, variable range, and number of variables. \n",
    "* Analyze why the failure occurs and suggest potential improvements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Break the code!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse598",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
